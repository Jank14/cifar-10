{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Hyperparameter Search in Tensorflow\n",
    "\n",
    "## 3rd Round\n",
    "In the first two rounds of hyperparameter searches, the stride of the first layer was fixed at 2. This was a necessity given the time required to train a network with a stride of 1 on a CPU (Google Compute Engine, here I come!). In this 3rd round I reduced the stride from 2 to 1 on the 1st layer.\n",
    "\n",
    "Having analysed the performance of 50 CNNs with randomly chosen hyperparameters, values which tended to result in poor performance were eliminated.\n",
    "\n",
    "The code in this notebook is virtually identical to that in the notebook used for the 1st two rounds. The main difference is the range of hyperparameter values that were sampled and the training logs at the end.\n",
    "\n",
    "The analysis for all rounds is carried out in a seperate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provided on https://www.cs.toronto.edu/~kriz/cifar.html \n",
    "# Given a \"pickled\" file, returns a dictionary containing the image data\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is stored in four files, which are read and merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (40000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for file_no in range(4):\n",
    "    # Obtain data dictionary from each file\n",
    "    filename = \"cifar-10-batches-py/data_batch_\" + str(file_no + 1)\n",
    "    image_batch = unpickle(filename)\n",
    "    # First file, create numpy arrays containing data & labels\n",
    "    if file_no == 0:\n",
    "        # Reshape to 32x32 image with 3 channels (RGB), which is made to be the last axes\n",
    "        image_data = image_batch[b'data'].reshape((-1,3,32,32)).transpose((0,2,3,1))\n",
    "        image_labels = image_batch[b'labels']\n",
    "    else: # Concatenate to one file\n",
    "        new_data = image_batch[b'data'].reshape((-1,3,32,32)).transpose((0,2,3,1))\n",
    "        image_data = np.concatenate([image_data, new_data])\n",
    "        image_labels = np.concatenate([image_labels, image_batch[b'labels']])\n",
    "\n",
    "print(\"Training data shape: \",image_data.shape)\n",
    "\n",
    "# Create numpy array containing test data\n",
    "test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "test_data = test_batch[b'data'].reshape((-1,3,32,32)).transpose((0,2,3,1))\n",
    "test_labels = test_batch[b'labels']\n",
    "\n",
    "# Obtain label names from the meta data\n",
    "label_names = unpickle(\"cifar-10-batches-py/batches.meta\")[b'label_names']\n",
    "label_names = [l.decode('UTF-8') for l in label_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of possible hyperparameter combinations is 2 X 1 X 4 X 2 X 2 X 2 X 2 X 1 X 1 X 3 X 2 X 1 X 2 X 2 = 3,072\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Fixed hyperparameters\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "outputs = 10\n",
    "\n",
    "n_epochs = 500\n",
    "# number of epochs to try to get a lower loss before stopping\n",
    "early_stop_rounds = 4\n",
    "\n",
    "# number of iterations, within epoch, to do an accuracy check\n",
    "acc_check = 25\n",
    "\n",
    "# Variable Hyperparameters\n",
    "hyperparam_range = {'filters1':[64, 96],\n",
    "                    'ksize1':[4, 5],\n",
    "                    'filters2':[96, 128],\n",
    "                    'ksize2':[4, 5],\n",
    "                    'filters3':[96, 128],\n",
    "                    'ksize3':[4, 5],\n",
    "                    'full_hidd1':[100, 125],\n",
    "                    'full_hidd2':[100, 125],\n",
    "                    'activation':['lrelu'],\n",
    "                    'learning_rate':[0.001, 0.0015, 0.002, 0.003],\n",
    "                    'batch_size':[64],\n",
    "                    'momentum':[0.9, 0.95, 0.99],\n",
    "                    'patch_reduction':[0],\n",
    "                    'optimizer':['adam']\n",
    "                   }\n",
    "\n",
    "# Calculate the number of hyperparamter grid points\n",
    "first_item = True\n",
    "for key,values in hyperparam_range.items():\n",
    "    count = len(values)\n",
    "    if first_item:\n",
    "        display = str(count)\n",
    "        total = count\n",
    "        first_item = False\n",
    "    else:\n",
    "        display = display + ' X ' + str(count)\n",
    "        total *= count\n",
    "\n",
    "print('The total number of possible hyperparameter combinations is ' + display + ' = ' + \"{:,}\".format(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(tensor_input, layer_no, filters, ksize, kstride, activation_unit, momentum, phase_train):\n",
    "    # convolutional layer with batch normalisation and max pooling\n",
    "    with tf.name_scope(\"conv_layer\" + str(layer_no)):\n",
    "        conv = tf.layers.conv2d(\n",
    "            tensor_input,\n",
    "            filters=filters,\n",
    "            kernel_size=ksize,\n",
    "            strides=[kstride,kstride],\n",
    "            padding=\"SAME\",\n",
    "            activation=None\n",
    "        )\n",
    "\n",
    "        conv_bn = tf.layers.batch_normalization(\n",
    "            inputs=conv,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=0.001,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            trainable=True,\n",
    "            training = phase_train\n",
    "        )\n",
    "\n",
    "        #apply activation unit\n",
    "        conv_bn_relu =  activation_unit(conv_bn)\n",
    "\n",
    "        max_pool = tf.nn.max_pool(\n",
    "            conv_bn_relu,\n",
    "            ksize=[1,2,2,1],\n",
    "            strides=[1,2,2,1],\n",
    "            padding=\"VALID\"\n",
    "        )\n",
    "        \n",
    "        dropout = tf.layers.dropout(\n",
    "            max_pool,\n",
    "            training = phase_train\n",
    "        )\n",
    "            \n",
    "            \n",
    "        return dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(hyperparam):\n",
    "    # Retrieve hyperparamaters from dictionary   \n",
    "    filters1 = hyperparam['filters1']\n",
    "    ksize1 = hyperparam['ksize1']\n",
    "    filters2 = hyperparam['filters2']\n",
    "    ksize2 = hyperparam['ksize2']\n",
    "    filters3 = hyperparam['filters3']\n",
    "    ksize3 = hyperparam['ksize3']\n",
    "    full_hidd1 = hyperparam['full_hidd1']\n",
    "    full_hidd2 = hyperparam['full_hidd2']\n",
    "    activation = hyperparam['activation']\n",
    "    learning_rate = hyperparam['learning_rate']\n",
    "    momentum = hyperparam['momentum']\n",
    "    patch_reduction =  hyperparam['patch_reduction']\n",
    "    optimizer_method = hyperparam['optimizer']\n",
    "    \n",
    "    patch_height = height - 2 * patch_reduction\n",
    "    patch_width = width - 2 * patch_reduction\n",
    "    \n",
    "    if activation == 'elu':\n",
    "        activation_unit = tf.nn.elu\n",
    "    elif activation == 'lrelu':\n",
    "        activation_unit = tf.nn.leaky_relu\n",
    "    else:    \n",
    "        activation_unit = tf.nn.relu\n",
    "        \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        X = tf.placeholder(shape=(None, patch_height, patch_width, channels), dtype=tf.float32)\n",
    "        y = tf.placeholder(shape=(None), dtype=tf.int32)\n",
    "        phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "        # 1st convolutional layer\n",
    "        conv1 = conv_layer(X, 1, filters1, ksize1, 1, activation_unit, momentum, phase_train)\n",
    "        \n",
    "        # 2nd convolutional layer\n",
    "        conv2 = conv_layer(conv1, 2, filters2, ksize2, 1, activation_unit, momentum, phase_train)\n",
    "        \n",
    "        # 3rd cnn\n",
    "        conv3 = conv_layer(conv2, 3, filters3, ksize3, 1, activation_unit, momentum, phase_train)\n",
    "        \n",
    "        # 1st fully connected (dense) layer\n",
    "        fully_conn1 = tf.layers.dense(conv3, full_hidd1, name=\"fully_conn1\", activation=activation_unit)\n",
    "        flat = tf.contrib.layers.flatten(fully_conn1)\n",
    "        \n",
    "        # 2nd fully connected layer\n",
    "        fully_conn2 = tf.layers.dense(flat, full_hidd2, name=\"fully_conn2\", activation=activation_unit)\n",
    "        \n",
    "        # Output layer (no activation function, as this is built into cross-entropy)\n",
    "        logits = tf.layers.dense(fully_conn2, outputs, name=\"logits\")\n",
    "\n",
    "        # Cross Entropy Loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        # Training    \n",
    "        with tf.name_scope(\"train\"):\n",
    "            if optimizer_method == 'rmsprop':\n",
    "                    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "            elif optimizer_method == 'nesterov':\n",
    "                    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov = True)\n",
    "            else:    \n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "    \n",
    "        # Initialization & Saver\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver() \n",
    "        \n",
    "        return graph, X, y, phase_train, logits, loss, training_op, init, saver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to train model, returns minimised loss\n",
    "def train_model(batch_size = 128):\n",
    "    training_size = image_data.shape[0]\n",
    "    no_batches = training_size // batch_size\n",
    "    \n",
    "    with graph.as_default():\n",
    "        # Ensure batch normalisation gets updated\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)    \n",
    "        # Add Evaluation metrics\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            acc_summary = tf.summary.scalar('Accuracy', accuracy)\n",
    "            loss_summary = tf.summary.scalar('Loss', loss)\n",
    "            \n",
    "    # Initialise best loss for early stopping\n",
    "    best_loss = 1e9\n",
    "    best_acc = 1e9\n",
    "    early_stopping = False\n",
    "    \n",
    "    # Run training\n",
    "    with tf.Session(graph=graph) as sess:                     \n",
    "        init.run()\n",
    "        epoch = 0\n",
    "        early_stop_count = 0\n",
    "        # Initialise training accuracy sum\n",
    "        sum_acc_train = 0.0\n",
    "        sum_loss_train = 0.0\n",
    "        count_train = 0\n",
    "        while (epoch < n_epochs) and (not early_stopping):\n",
    "            epoch += 1\n",
    "            # Shuffle training data\n",
    "            p = np.random.permutation(training_size)\n",
    "            image_data_shuffle = image_data_patch[p]\n",
    "            image_labels_shuffle = image_labels[p]\n",
    "            for iteration in range(no_batches-1):\n",
    "                # Train on mini-batch\n",
    "                X_batch = image_data_shuffle[iteration * batch_size:(iteration + 1) * batch_size + 1]\n",
    "                y_batch = image_labels_shuffle[iteration * batch_size:(iteration + 1) * batch_size + 1]\n",
    "                sess.run([training_op, extra_update_ops], feed_dict={X: X_batch, y: y_batch, phase_train: True})\n",
    "                # Evaluate model on current mini-batch\n",
    "                if (iteration % acc_check == 0):\n",
    "                    acc_train, loss_train = sess.run([accuracy, loss], feed_dict={X: X_batch,\n",
    "                                                                                   y: y_batch,\n",
    "                                                                                   phase_train: True})\n",
    "                    # Update training average for current epoch\n",
    "                    sum_acc_train += acc_train\n",
    "                    sum_loss_train += loss_train\n",
    "                    count_train += 1\n",
    "                    mean_acc_train = sum_acc_train / count_train\n",
    "                    mean_loss_train = sum_loss_train / count_train\n",
    "                    print(epoch, iteration, \"Train accuracy:\", mean_acc_train, \" Train loss:\", mean_loss_train, end='\\r')\n",
    "                     \n",
    "            # Print mean Train accuracy        \n",
    "            print(epoch, iteration, \"Train accuracy:\", mean_acc_train, \" Train loss:\", mean_loss_train)\n",
    "            train_summary = tf.Summary()\n",
    "            train_summary.value.add(tag='eval/Accuracy', simple_value=mean_acc_train)\n",
    "            train_summary.value.add(tag='eval/Loss', simple_value=mean_loss_train)\n",
    "            file_writer_train.add_summary(train_summary, epoch)\n",
    "            \n",
    "            # Evaluate model on test data every epoch\n",
    "            acc_test, loss_test, summary_str_acc_test, summary_str_loss_test = sess.run([accuracy, loss,\n",
    "                                                                                         acc_summary,\n",
    "                                                                                         loss_summary],\n",
    "                                                                                        feed_dict={X: test_data_patch,\n",
    "                                                                                                   y: test_labels,\n",
    "                                                                                                   phase_train: False})\n",
    "            # Check for mininimum loss\n",
    "            if loss_test < best_loss:\n",
    "                # Reset early stopping best loss and count\n",
    "                best_loss = loss_test\n",
    "                best_acc = acc_test\n",
    "                best_train_acc = mean_acc_train\n",
    "                best_train_loss = mean_loss_train\n",
    "                early_stop_count = 0\n",
    "                # Save model\n",
    "                save_path = saver.save(sess, ckptfile)\n",
    "            else:\n",
    "                # Increment early stopping count\n",
    "                early_stop_count += 1\n",
    "            # Check if sufficient rounds (epochs) have passed without improvement\n",
    "            if early_stop_count  > early_stop_rounds:\n",
    "                # Flag early stopping, so training loop will stop\n",
    "                early_stopping = True\n",
    "                print(\"Early stopping, best loss: \", best_loss)\n",
    "            else:    \n",
    "                # Record and display test loss and accuracy\n",
    "                file_writer_test.add_summary(summary_str_acc_test, epoch)\n",
    "                file_writer_test.add_summary(summary_str_loss_test, epoch)\n",
    "                print(epoch, \"Test accuracy:\", acc_test, \" Test loss:\", loss_test)\n",
    "            \n",
    "        # Return Best Loss obtaned and number of epochs\n",
    "        return best_loss, best_acc, best_train_acc, best_train_loss, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Hyperparameter Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with following hyperparameters:\n",
      "{'ksize1': 4, 'patch_reduction': 0, 'learning_rate': 0.001, 'ksize2': 5, 'ksize3': 5, 'filters1': 64, 'full_hidd2': 100, 'batch_size': 64, 'filters3': 96, 'activation': 'lrelu', 'momentum': 0.95, 'optimizer': 'adam', 'filters2': 96, 'full_hidd1': 125}\n",
      "1 623 Train accuracy: 0.402461548746  Train loss: 1.63585497856\n",
      "1 Test accuracy: 0.41  Test loss: 1.76227\n",
      "2 623 Train accuracy: 0.473846169561  Train loss: 1.43774188995\n",
      "2 Test accuracy: 0.5482  Test loss: 1.27906\n",
      "3 623 Train accuracy: 0.517128223081  Train loss: 1.32683193684\n",
      "3 Test accuracy: 0.5799  Test loss: 1.15588\n",
      "4 623 Train accuracy: 0.548461557254  Train loss: 1.24980944633\n",
      "4 Test accuracy: 0.654  Test loss: 0.96421\n",
      "5 623 Train accuracy: 0.573907711208  Train loss: 1.18282833958\n",
      "5 Test accuracy: 0.624  Test loss: 1.06843\n",
      "6 623 Train accuracy: 0.590564121554  Train loss: 1.13227905432\n",
      "6 Test accuracy: 0.7017  Test loss: 0.846809\n",
      "7 623 Train accuracy: 0.607032985815  Train loss: 1.09171028716\n",
      "7 Test accuracy: 0.6738  Test loss: 0.934429\n",
      "8 623 Train accuracy: 0.620461557023  Train loss: 1.05608430356\n",
      "8 Test accuracy: 0.6837  Test loss: 0.930079\n",
      "9 623 Train accuracy: 0.631931642261  Train loss: 1.02229827642\n",
      "9 Test accuracy: 0.6679  Test loss: 0.968338\n",
      "10 623 Train accuracy: 0.643876941055  Train loss: 0.990058308363\n",
      "10 Test accuracy: 0.7045  Test loss: 0.862004\n",
      "11 623 Train accuracy: 0.653426591131  Train loss: 0.964523540952\n",
      "11 Test accuracy: 0.739  Test loss: 0.74815\n",
      "12 623 Train accuracy: 0.662358991777  Train loss: 0.940491941472\n",
      "12 Test accuracy: 0.739  Test loss: 0.744716\n",
      "13 623 Train accuracy: 0.670343212417  Train loss: 0.917541624399\n",
      "13 Test accuracy: 0.7058  Test loss: 0.854773\n",
      "14 623 Train accuracy: 0.676483533446  Train loss: 0.901767110484\n",
      "14 Test accuracy: 0.7122  Test loss: 0.847137\n",
      "15 623 Train accuracy: 0.683774375657  Train loss: 0.883369219764\n",
      "15 Test accuracy: 0.7641  Test loss: 0.677325\n",
      "16 623 Train accuracy: 0.690307708737  Train loss: 0.865797477293\n",
      "16 Test accuracy: 0.7766  Test loss: 0.655008\n",
      "17 623 Train accuracy: 0.695963817116  Train loss: 0.849651826199\n",
      "17 Test accuracy: 0.729  Test loss: 0.827669\n",
      "18 623 Train accuracy: 0.70085471688  Train loss: 0.8363247887964\n",
      "18 Test accuracy: 0.7847  Test loss: 0.637285\n",
      "19 623 Train accuracy: 0.705554671711  Train loss: 0.822717470119\n",
      "19 Test accuracy: 0.7786  Test loss: 0.645443\n",
      "20 623 Train accuracy: 0.71003078489  Train loss: 0.8090769740649\n",
      "20 Test accuracy: 0.7384  Test loss: 0.815753\n",
      "21 623 Train accuracy: 0.714930418375  Train loss: 0.794580973415\n",
      "21 Test accuracy: 0.7799  Test loss: 0.632761\n",
      "22 623 Train accuracy: 0.718937078213  Train loss: 0.783606952998\n",
      "22 Test accuracy: 0.7642  Test loss: 0.699819\n",
      "23 623 Train accuracy: 0.722568576997  Train loss: 0.774172679989\n",
      "23 Test accuracy: 0.7712  Test loss: 0.682395\n",
      "24 623 Train accuracy: 0.725846168833  Train loss: 0.765326953207\n",
      "24 Test accuracy: 0.765  Test loss: 0.70626\n",
      "25 623 Train accuracy: 0.729132322538  Train loss: 0.756271959901\n",
      "25 Test accuracy: 0.7771  Test loss: 0.675653\n",
      "26 623 Train accuracy: 0.732189363826  Train loss: 0.747390702573\n",
      "Early stopping, best loss:  0.632761\n",
      "Training Model with following hyperparameters:\n",
      "{'ksize1': 5, 'patch_reduction': 0, 'learning_rate': 0.0015, 'ksize2': 4, 'ksize3': 5, 'filters1': 64, 'full_hidd2': 125, 'batch_size': 64, 'filters3': 128, 'activation': 'lrelu', 'momentum': 0.95, 'optimizer': 'adam', 'filters2': 96, 'full_hidd1': 100}\n",
      "1 623 Train accuracy: 0.409230779409  Train loss: 1.64000178337\n",
      "1 Test accuracy: 0.4796  Test loss: 1.45752\n",
      "2 623 Train accuracy: 0.483692323565  Train loss: 1.43848528981\n",
      "2 Test accuracy: 0.4863  Test loss: 1.52475\n",
      "3 623 Train accuracy: 0.522666683992  Train loss: 1.32892257373\n",
      "3 Test accuracy: 0.6244  Test loss: 1.03138\n",
      "4 623 Train accuracy: 0.555230787098  Train loss: 1.24172924817\n",
      "4 Test accuracy: 0.6327  Test loss: 1.03843\n",
      "5 623 Train accuracy: 0.577846172094  Train loss: 1.17904905519\n",
      "5 Test accuracy: 0.6263  Test loss: 1.07032\n",
      "6 623 Train accuracy: 0.598769248923  Train loss: 1.12760511518\n",
      "6 Test accuracy: 0.7134  Test loss: 0.824861\n",
      "7 623 Train accuracy: 0.613450567552  Train loss: 1.08796114138\n",
      "7 Test accuracy: 0.6483  Test loss: 1.05829\n",
      "8 623 Train accuracy: 0.627153864056  Train loss: 1.04711901233\n",
      "8 Test accuracy: 0.6978  Test loss: 0.860839\n",
      "9 623 Train accuracy: 0.638906000588  Train loss: 1.01403799076\n",
      "9 Test accuracy: 0.704  Test loss: 0.862383\n",
      "10 623 Train accuracy: 0.652184632659  Train loss: 0.979283409715\n",
      "10 Test accuracy: 0.7405  Test loss: 0.737845\n",
      "11 623 Train accuracy: 0.661706310727  Train loss: 0.953206715475\n",
      "11 Test accuracy: 0.7486  Test loss: 0.737396\n",
      "12 623 Train accuracy: 0.67112822185  Train loss: 0.9257048913843\n",
      "12 Test accuracy: 0.73  Test loss: 0.791902\n",
      "13 623 Train accuracy: 0.677775164476  Train loss: 0.905661580654\n",
      "13 Test accuracy: 0.6469  Test loss: 1.08447\n",
      "14 623 Train accuracy: 0.684263752614  Train loss: 0.887534710254\n",
      "14 Test accuracy: 0.6735  Test loss: 1.02285\n",
      "15 623 Train accuracy: 0.690830785354  Train loss: 0.867991925955\n",
      "15 Test accuracy: 0.7341  Test loss: 0.81585\n",
      "16 623 Train accuracy: 0.697230785117  Train loss: 0.850534543097\n",
      "16 Test accuracy: 0.775  Test loss: 0.649881\n",
      "17 623 Train accuracy: 0.704108612888  Train loss: 0.832614244994\n",
      "17 Test accuracy: 0.6925  Test loss: 0.9803\n",
      "18 600 Train accuracy: 0.709128220545  Train loss: 0.817314759493623 Train accuracy: 0.709128220545  Train loss: 0.817314759493\n",
      "18 Test accuracy: 0.738  Test loss: 0.802703\n",
      "19 623 Train accuracy: 0.714299610351  Train loss: 0.803695879359\n",
      "19 Test accuracy: 0.7265  Test loss: 0.814068\n",
      "20 623 Train accuracy: 0.718953861177  Train loss: 0.790938818932\n",
      "20 Test accuracy: 0.6997  Test loss: 1.00834\n",
      "21 623 Train accuracy: 0.723838842653  Train loss: 0.777566736199\n",
      "Early stopping, best loss:  0.649881\n",
      "Training Model with following hyperparameters:\n",
      "{'ksize1': 4, 'patch_reduction': 0, 'learning_rate': 0.001, 'ksize2': 5, 'ksize3': 5, 'filters1': 96, 'full_hidd2': 125, 'batch_size': 64, 'filters3': 128, 'activation': 'lrelu', 'momentum': 0.95, 'optimizer': 'adam', 'filters2': 96, 'full_hidd1': 125}\n",
      "1 623 Train accuracy: 0.435692318082  Train loss: 1.60747704029\n",
      "1 Test accuracy: 0.5184  Test loss: 1.3315\n",
      "2 623 Train accuracy: 0.498769247234  Train loss: 1.41931112528\n",
      "2 Test accuracy: 0.6032  Test loss: 1.1068\n",
      "3 623 Train accuracy: 0.531282069882  Train loss: 1.30263843139\n",
      "3 Test accuracy: 0.5268  Test loss: 1.39049\n",
      "4 623 Train accuracy: 0.568615403324  Train loss: 1.21237069666\n",
      "4 Test accuracy: 0.6216  Test loss: 1.11693\n",
      "5 623 Train accuracy: 0.591507711053  Train loss: 1.14567852592\n",
      "5 Test accuracy: 0.6738  Test loss: 0.940117\n",
      "6 623 Train accuracy: 0.612923095326  Train loss: 1.09046458483\n",
      "6 Test accuracy: 0.6525  Test loss: 1.04505\n",
      "7 623 Train accuracy: 0.628571446708  Train loss: 1.04986017363\n",
      "7 Test accuracy: 0.7103  Test loss: 0.818456\n",
      "8 623 Train accuracy: 0.641923094764  Train loss: 1.00903837771\n",
      "8 Test accuracy: 0.6893  Test loss: 0.898774\n",
      "9 623 Train accuracy: 0.653196598755  Train loss: 0.976740395493\n",
      "9 Test accuracy: 0.7199  Test loss: 0.799646\n",
      "10 623 Train accuracy: 0.66387694031  Train loss: 0.9468899236926\n",
      "10 Test accuracy: 0.7359  Test loss: 0.782512\n",
      "11 623 Train accuracy: 0.673342674266  Train loss: 0.920014627955\n",
      "11 Test accuracy: 0.7416  Test loss: 0.747863\n",
      "12 623 Train accuracy: 0.683128221681  Train loss: 0.891300521692\n",
      "12 Test accuracy: 0.7558  Test loss: 0.708895\n",
      "13 623 Train accuracy: 0.691171613886  Train loss: 0.866610339055\n",
      "13 Test accuracy: 0.7665  Test loss: 0.684781\n",
      "14 623 Train accuracy: 0.698285730268  Train loss: 0.846880062308\n",
      "14 Test accuracy: 0.7481  Test loss: 0.744763\n",
      "15 623 Train accuracy: 0.705394887487  Train loss: 0.826825923363\n",
      "15 Test accuracy: 0.7675  Test loss: 0.69161\n",
      "16 623 Train accuracy: 0.711807707734  Train loss: 0.809539821446\n",
      "16 Test accuracy: 0.758  Test loss: 0.724414\n",
      "17 623 Train accuracy: 0.717212684891  Train loss: 0.793946913481\n",
      "17 Test accuracy: 0.7831  Test loss: 0.632849\n",
      "18 623 Train accuracy: 0.722529929512  Train loss: 0.779549190534\n",
      "18 Test accuracy: 0.7794  Test loss: 0.644159\n",
      "19 600 Train accuracy: 0.727773294104  Train loss: 0.765125100738623 Train accuracy: 0.727773294104  Train loss: 0.765125100738\n",
      "19 Test accuracy: 0.7599  Test loss: 0.742068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 623 Train accuracy: 0.731876937658  Train loss: 0.753798472881\n",
      "20 Test accuracy: 0.7917  Test loss: 0.624902\n",
      "21 623 Train accuracy: 0.735882798291  Train loss: 0.743288304068\n",
      "21 Test accuracy: 0.767  Test loss: 0.714343\n",
      "22 623 Train accuracy: 0.739972042198  Train loss: 0.731531067978\n",
      "22 Test accuracy: 0.7834  Test loss: 0.667931\n",
      "23 623 Train accuracy: 0.744026769892  Train loss: 0.720355961536\n",
      "23 Test accuracy: 0.7948  Test loss: 0.614702\n",
      "24 623 Train accuracy: 0.747333347226  Train loss: 0.711331373453\n",
      "24 Test accuracy: 0.8051  Test loss: 0.58721\n",
      "25 623 Train accuracy: 0.750572321439  Train loss: 0.701923216057\n",
      "25 Test accuracy: 0.7919  Test loss: 0.633341\n",
      "26 623 Train accuracy: 0.754011847904  Train loss: 0.692114287523\n",
      "26 Test accuracy: 0.7956  Test loss: 0.641457\n",
      "27 623 Train accuracy: 0.757743603146  Train loss: 0.682787806789\n",
      "27 Test accuracy: 0.7333  Test loss: 0.877364\n",
      "28 623 Train accuracy: 0.7611648484  Train loss: 0.67330351789154\n",
      "28 Test accuracy: 0.8048  Test loss: 0.587198\n",
      "29 623 Train accuracy: 0.764626007758  Train loss: 0.664436435725\n",
      "29 Test accuracy: 0.7524  Test loss: 0.802526\n",
      "30 623 Train accuracy: 0.766892320653  Train loss: 0.656999752741\n",
      "30 Test accuracy: 0.8085  Test loss: 0.593728\n",
      "31 623 Train accuracy: 0.769310186544  Train loss: 0.649615428467\n",
      "31 Test accuracy: 0.7595  Test loss: 0.800232\n",
      "32 623 Train accuracy: 0.771961551178  Train loss: 0.642172379643\n",
      "32 Test accuracy: 0.7931  Test loss: 0.650019\n",
      "33 623 Train accuracy: 0.774452227047  Train loss: 0.634839963841\n",
      "Early stopping, best loss:  0.587198\n",
      "Training Model with following hyperparameters:\n",
      "{'ksize1': 4, 'patch_reduction': 0, 'learning_rate': 0.003, 'ksize2': 5, 'ksize3': 5, 'filters1': 96, 'full_hidd2': 100, 'batch_size': 64, 'filters3': 128, 'activation': 'lrelu', 'momentum': 0.95, 'optimizer': 'adam', 'filters2': 128, 'full_hidd1': 125}\n",
      "1 600 Train accuracy: 0.413538469672  Train loss: 1.72801503181623 Train accuracy: 0.413538469672  Train loss: 1.72801503181\n",
      "1 Test accuracy: 0.4131  Test loss: 1.75982\n",
      "2 623 Train accuracy: 0.492000014484  Train loss: 1.47088844538\n",
      "2 Test accuracy: 0.4715  Test loss: 1.49798\n",
      "3 623 Train accuracy: 0.542153862913  Train loss: 1.30963759979\n",
      "3 Test accuracy: 0.6526  Test loss: 0.97569\n",
      "4 623 Train accuracy: 0.574000017494  Train loss: 1.20952440083\n",
      "4 Test accuracy: 0.6787  Test loss: 0.927079\n",
      "5 623 Train accuracy: 0.597046171308  Train loss: 1.13770901108\n",
      "5 Test accuracy: 0.6373  Test loss: 1.08898\n",
      "6 623 Train accuracy: 0.62133335044  Train loss: 1.073945595822\n",
      "6 Test accuracy: 0.7035  Test loss: 0.8433\n",
      "7 623 Train accuracy: 0.635076940145  Train loss: 1.03406102794\n",
      "7 Test accuracy: 0.7355  Test loss: 0.757713\n",
      "8 623 Train accuracy: 0.646076940075  Train loss: 1.00185199589\n",
      "8 Test accuracy: 0.731  Test loss: 0.786283\n",
      "9 600 Train accuracy: 0.657504290276  Train loss: 0.973132497474623 Train accuracy: 0.657504290276  Train loss: 0.97313249747\n",
      "9 Test accuracy: 0.7483  Test loss: 0.731329\n",
      "10 623 Train accuracy: 0.666584631979  Train loss: 0.948467275513\n",
      "10 Test accuracy: 0.7166  Test loss: 0.823372\n",
      "11 623 Train accuracy: 0.675356659727  Train loss: 0.923461653861\n",
      "11 Test accuracy: 0.7403  Test loss: 0.754908\n",
      "12 623 Train accuracy: 0.681948734174  Train loss: 0.907571005126\n",
      "12 Test accuracy: 0.7154  Test loss: 0.846587\n",
      "13 623 Train accuracy: 0.689136110682  Train loss: 0.887134800232\n",
      "13 Test accuracy: 0.7123  Test loss: 0.863928\n",
      "14 623 Train accuracy: 0.696967048688  Train loss: 0.867366472398\n",
      "14 Test accuracy: 0.7526  Test loss: 0.724414\n",
      "15 623 Train accuracy: 0.70334360524  Train loss: 0.8490573950619\n",
      "15 Test accuracy: 0.7541  Test loss: 0.738622\n",
      "16 623 Train accuracy: 0.708076938428  Train loss: 0.834882441238\n",
      "16 Test accuracy: 0.7708  Test loss: 0.679167\n",
      "17 600 Train accuracy: 0.712687798002  Train loss: 0.819579425209623 Train accuracy: 0.712687798002  Train loss: 0.819579425209\n",
      "17 Test accuracy: 0.6577  Test loss: 1.14585\n",
      "18 623 Train accuracy: 0.717914544907  Train loss: 0.806051905817\n",
      "18 Test accuracy: 0.7852  Test loss: 0.654355\n",
      "19 623 Train accuracy: 0.72200811201  Train loss: 0.7939361188297\n",
      "19 Test accuracy: 0.7395  Test loss: 0.804105\n",
      "20 623 Train accuracy: 0.726400014669  Train loss: 0.781413454711\n",
      "20 Test accuracy: 0.757  Test loss: 0.72945\n",
      "21 623 Train accuracy: 0.730168512691  Train loss: 0.770988448688\n",
      "21 Test accuracy: 0.7268  Test loss: 0.930537\n",
      "22 623 Train accuracy: 0.733930084299  Train loss: 0.761124033928\n",
      "22 Test accuracy: 0.7811  Test loss: 0.649493\n",
      "23 623 Train accuracy: 0.736909713253  Train loss: 0.751419904439\n",
      "23 Test accuracy: 0.8062  Test loss: 0.579502\n",
      "24 623 Train accuracy: 0.740076937204  Train loss: 0.742507108748\n",
      "24 Test accuracy: 0.7729  Test loss: 0.677108\n",
      "25 623 Train accuracy: 0.743040014005  Train loss: 0.734112475061\n",
      "25 Test accuracy: 0.7551  Test loss: 0.7646\n",
      "26 623 Train accuracy: 0.745633149995  Train loss: 0.726298235936\n",
      "26 Test accuracy: 0.7756  Test loss: 0.671759\n",
      "27 623 Train accuracy: 0.748125369924  Train loss: 0.719191595978\n",
      "27 Test accuracy: 0.7869  Test loss: 0.66522\n",
      "28 600 Train accuracy: 0.750549464247  Train loss: 0.711729701502623 Train accuracy: 0.750549464247  Train loss: 0.711729701502\n",
      "Early stopping, best loss:  0.579502\n",
      "Training Model with following hyperparameters:\n",
      "{'ksize1': 5, 'patch_reduction': 0, 'learning_rate': 0.003, 'ksize2': 5, 'ksize3': 5, 'filters1': 64, 'full_hidd2': 100, 'batch_size': 64, 'filters3': 128, 'activation': 'lrelu', 'momentum': 0.9, 'optimizer': 'adam', 'filters2': 128, 'full_hidd1': 100}\n",
      "1 623 Train accuracy: 0.430153856874  Train loss: 1.80154718876\n",
      "1 Test accuracy: 0.4647  Test loss: 1.49974\n",
      "2 623 Train accuracy: 0.495384630859  Train loss: 1.51928786516\n",
      "2 Test accuracy: 0.5694  Test loss: 1.22344\n",
      "3 623 Train accuracy: 0.537846171657  Train loss: 1.37213389953\n",
      "3 Test accuracy: 0.5738  Test loss: 1.22898\n",
      "4 600 Train accuracy: 0.569692326039  Train loss: 1.26225036919623 Train accuracy: 0.569692326039  Train loss: 1.26225036919\n",
      "4 Test accuracy: 0.6862  Test loss: 0.901165\n",
      "5 623 Train accuracy: 0.589169249415  Train loss: 1.19629534292\n",
      "5 Test accuracy: 0.6281  Test loss: 1.0847\n",
      "6 623 Train accuracy: 0.608923095365  Train loss: 1.13392838836\n",
      "6 Test accuracy: 0.6553  Test loss: 1.01021\n",
      "7 623 Train accuracy: 0.625758259892  Train loss: 1.08495020567\n",
      "7 Test accuracy: 0.7041  Test loss: 0.84385\n",
      "8 623 Train accuracy: 0.639461556301  Train loss: 1.04676324695\n",
      "8 Test accuracy: 0.7138  Test loss: 0.822197\n",
      "9 623 Train accuracy: 0.650324803922  Train loss: 1.01117461922\n",
      "9 Test accuracy: 0.7391  Test loss: 0.757786\n",
      "10 623 Train accuracy: 0.660800017297  Train loss: 0.979996391892\n",
      "10 Test accuracy: 0.6803  Test loss: 0.953411\n",
      "11 623 Train accuracy: 0.669034982107  Train loss: 0.953694098863\n",
      "11 Test accuracy: 0.7445  Test loss: 0.748155\n",
      "12 623 Train accuracy: 0.677794888566  Train loss: 0.927559084694\n",
      "12 Test accuracy: 0.7149  Test loss: 0.85505\n",
      "13 623 Train accuracy: 0.686248537164  Train loss: 0.903107179128\n",
      "13 Test accuracy: 0.7064  Test loss: 0.876897\n",
      "14 623 Train accuracy: 0.692307708561  Train loss: 0.886014713645\n",
      "14 Test accuracy: 0.7475  Test loss: 0.769914\n",
      "15 623 Train accuracy: 0.698051298102  Train loss: 0.867963595549\n",
      "15 Test accuracy: 0.7462  Test loss: 0.751515\n",
      "16 623 Train accuracy: 0.702615400515  Train loss: 0.852848301679\n",
      "16 Test accuracy: 0.7619  Test loss: 0.702908\n",
      "17 623 Train accuracy: 0.708380106162  Train loss: 0.835695354868\n",
      "17 Test accuracy: 0.7575  Test loss: 0.703064\n",
      "18 623 Train accuracy: 0.712854716347  Train loss: 0.821939857403\n",
      "18 Test accuracy: 0.7814  Test loss: 0.661598\n",
      "19 600 Train accuracy: 0.717473699513  Train loss: 0.809084403954623 Train accuracy: 0.717473699513  Train loss: 0.809084403954\n",
      "19 Test accuracy: 0.7107  Test loss: 0.922034\n",
      "20 623 Train accuracy: 0.721076938242  Train loss: 0.797817967355\n",
      "20 Test accuracy: 0.7894  Test loss: 0.622922\n",
      "21 623 Train accuracy: 0.724571443597  Train loss: 0.787304378407\n",
      "21 Test accuracy: 0.7422  Test loss: 0.782186\n",
      "22 623 Train accuracy: 0.728139875016  Train loss: 0.776668269147\n",
      "22 Test accuracy: 0.7115  Test loss: 0.918598\n",
      "23 623 Train accuracy: 0.731398008051  Train loss: 0.766863101254\n",
      "23 Test accuracy: 0.7733  Test loss: 0.683023\n",
      "24 623 Train accuracy: 0.734564117168  Train loss: 0.757698368728\n",
      "24 Test accuracy: 0.7395  Test loss: 0.800556\n",
      "25 623 Train accuracy: 0.737181552958  Train loss: 0.749989327192\n",
      "Early stopping, best loss:  0.622922\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "import os.path\n",
    "if not os.path.exists('./log'):\n",
    "    os.makedirs('./log')\n",
    "if not os.path.exists('./graphs'):\n",
    "    os.makedirs('./graphs')\n",
    "    \n",
    "if(os.path.isfile('./log/model_hyperparam_3.csv')):\n",
    "    hyperparam_df = pd.read_csv('./log/model_hyperparam_3.csv')\n",
    "    new_results_file = False\n",
    "else:\n",
    "    new_results_file = True\n",
    "\n",
    "# Random Model search\n",
    "for n in range(5):\n",
    "    # Create log directory using current timestamp\n",
    "    from datetime import datetime\n",
    "    now = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    root_logdir = \"./log\"\n",
    "    root_graphdir = \"./graphs\"\n",
    "    logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "    graphdir = \"{}/run-{}/\".format(root_graphdir, now)\n",
    "    #root_modeldir = \"./model\"\n",
    "    graphfile = graphdir\n",
    "    ckptfile = graphdir + \"checkpoint.ckpt\"\n",
    "\n",
    "    # Note that the graph is not written using same filewriter as the logging data.\n",
    "    # This allows the logs to be viewed during training before the filewriter is closed\n",
    "    file_writer_train = tf.summary.FileWriter(logdir + '/train')\n",
    "    file_writer_test = tf.summary.FileWriter(logdir + '/test')\n",
    "    \n",
    "    # tensorboard --logdir e:\\Programming\\TensorFlow\\CIFAR-10\\log\n",
    "    # The command must be executed from same drive (E:) as the logdir\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    hyperparam_dict = {key:random.choice(values) for (key,values) in hyperparam_range.items()}\n",
    "    print(\"Training Model with following hyperparameters:\")\n",
    "    print(hyperparam_dict)\n",
    "\n",
    "    # Obtain graph and nodes required for training\n",
    "    graph, X, y, phase_train, logits, loss, training_op, init, saver = build_graph(hyperparam = hyperparam_dict)\n",
    "    \n",
    "    # Write graph to file and close to avoid TensorBoard conflict\n",
    "    file_writer_graph = tf.summary.FileWriter(graphfile, graph)\n",
    "    file_writer_graph.close()\n",
    "\n",
    "    # Extract image patches\n",
    "    patch_reduction =  hyperparam_dict['patch_reduction']\n",
    "    if patch_reduction == 0:\n",
    "        image_data_patch = image_data\n",
    "        test_data_patch = test_data\n",
    "    else:\n",
    "        image_data_patch = image_data[:,patch_reduction:-patch_reduction,patch_reduction:-patch_reduction,:]\n",
    "        test_data_patch = test_data[:,patch_reduction:-patch_reduction,patch_reduction:-patch_reduction,:]\n",
    "    \n",
    "    # Train the model\n",
    "    best_loss, best_acc, best_train_acc, best_train_loss, no_epochs = train_model(batch_size = hyperparam_dict['batch_size'])\n",
    "    \n",
    "    # Add results to dictionary\n",
    "    hyperparam_dict['best_loss'] = best_loss\n",
    "    hyperparam_dict['best_acc'] = best_acc\n",
    "    hyperparam_dict['best_train_acc'] = best_train_acc\n",
    "    hyperparam_dict['best_train_loss'] = best_train_loss\n",
    "    hyperparam_dict['no_epochs'] = no_epochs\n",
    "    hyperparam_dict['logdir'] = logdir\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    if new_results_file:\n",
    "        hyperparam_df = pd.DataFrame(hyperparam_dict, index=[0])\n",
    "        new_results_file = False\n",
    "    else:\n",
    "        hyperparam_df = hyperparam_df.append(hyperparam_dict, ignore_index=True)\n",
    "        \n",
    "    # Write results file (don't wait until end, as next model may be interrupted)\n",
    "    hyperparam_df.to_csv('./log/model_hyperparam_3.csv', index=False)\n",
    "\n",
    "    # Close filewriters\n",
    "    file_writer_train.close()\n",
    "    file_writer_test.close()\n",
    "    \n",
    "    # NOTE - From this point onwards there are only training logs.\n",
    "    # Results and Conclusion are in a seperate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
